{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM1M1Smpe0RZpI0VrojTg+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salsaadityani/natural-language-processing/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-GRAM**"
      ],
      "metadata": {
        "id": "m6hIAGhcpZRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9yyayKTngXP",
        "outputId": "372595d8-3721-4c5c-9351-07c931e48505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter(text):\n",
        "    # normalize text\n",
        "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "    # replace html chars with ' '\n",
        "    text = re.sub('<.*?>', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    # only alphabets and numerics\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # replace newline with space\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # split and join the words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Iz_4lTd6nrIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize remaining words and perform lemmatization\n",
        "def clean(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    output = []\n",
        "    for words in tokens:\n",
        "        # lemmatize words\n",
        "        output.append(wnl.lemmatize(words))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "yYLudiaJnvab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a language model using a dictionary, trigrams, and calculate word probabilities\n",
        "def n_gram_model(text):\n",
        "    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "    cfdist = ConditionalFreqDist()\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        cfdist[(w1, w2)][w3] += 1\n",
        "\n",
        "    # transform frequencies to probabilities\n",
        "    for w1_w2 in cfdist:\n",
        "        total_count = float(sum(cfdist[w1_w2].values()))\n",
        "        for w3 in cfdist[w1_w2]:\n",
        "            cfdist[w1_w2][w3] /= total_count\n",
        "\n",
        "    return cfdist"
      ],
      "metadata": {
        "id": "7I1pN3bxn0Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, user_input):\n",
        "    user_input = filter(user_input)\n",
        "    user_input = user_input.split()\n",
        "\n",
        "    w1 = len(user_input) - 2\n",
        "    w2 = len(user_input)\n",
        "    prev_words = user_input[w1:w2]\n",
        "\n",
        "    # display prediction from highest to lowest maximum likelihood\n",
        "    prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n",
        "    print(\"Trigram model predictions: \", prediction)\n",
        "\n",
        "    word = []\n",
        "    weight = []\n",
        "    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n",
        "        word.append(key)\n",
        "        weight.append(prob)\n",
        "    # pick from a weighted random probability of predictions\n",
        "    next_word = random.choices(word, weights=weight, k=1)\n",
        "    # add predicted word to user input\n",
        "    user_input.append(next_word[0])\n",
        "    print(' '.join(user_input))\n",
        "\n",
        "    ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "    if ask.lower() == 'y':\n",
        "        predict(model, str(user_input))\n",
        "    elif ask.lower() == 'n':\n",
        "        print(\"done\")"
      ],
      "metadata": {
        "id": "Nt4FEisVn-Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "\n",
        "file = open('alice_in_wonderland.txt', 'r')\n",
        "text = \"\"\n",
        "while True:\n",
        "  line = file.readline()\n",
        "  text += line\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "# pre-process text\n",
        "print(\"Filtering...\")\n",
        "words = filter(text)\n",
        "print(\"Cleaning...\")\n",
        "words = clean(words)\n",
        "\n",
        "# make language model\n",
        "print(\"Making model...\")\n",
        "model = n_gram_model(words)\n",
        "\n",
        "# example: \"alice said to the\"\n",
        "print(\"Enter a phrase: \")\n",
        "user_input = input()\n",
        "predict(model, user_input)"
      ],
      "metadata": {
        "id": "fOOf5WXvoFLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0747b5-ecdb-4892-d1c7-65d21a85555b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering...\n",
            "Cleaning...\n",
            "Making model...\n",
            "Enter a phrase: \n",
            "alice said to the\n",
            "Trigram model predictions:  ['project', 'other', 'jury', 'door', 'table', 'knave', 'mock', 'gryphon', 'full', 'little', 'end', 'shore', 'beginning', 'dormouse', 'game', 'queen', 'term', 'garden', 'seaside', 'general', 'cur', 'fifth', 'puppy', 'law', 'baby', 'three', 'king', 'rosetree', 'conclusion', 'cheshire', 'duchess', 'executioner', 'croquetground', 'company', 'classic', 'whiting', 'dance', 'porpoise', 'part', 'caterpillar', 'hatter', 'head', 'tart', 'waving', 'voice', 'confused', 'trademark', 'user', 'owner', 'person']\n",
            "alice said to the trademark\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['owner', 'license']\n",
            "alice said to the trademark owner\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): n\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "EeZHjaYbph5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Activation\n",
        "#from keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "DKFtdfdqpmkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 100\n",
        "\n",
        "def buildmodel(VOCABULARY):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dense(VOCABULARY, activation = 'softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "21_i3-mrqCjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('alice_in_wonderland.txt', encoding = 'utf8')\n",
        "raw_text = file.read()    #you need to read further characters as well\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "0_t8UZ05qGOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_chars = ['#', '*', '@', '_', '\\ufeff']\n",
        "for i in range(len(bad_chars)):\n",
        "    raw_text = raw_text.replace(bad_chars[i],\"\")"
      ],
      "metadata": {
        "id": "USDtbT5KqOck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZnHlJaUqLIx",
        "outputId": "8e472008-1a9a-4a67-d09c-ac78cdc345d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '$', '%', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_length = len(raw_text)\n",
        "char_length = len(chars)\n",
        "VOCABULARY = char_length\n",
        "print(\"Text length = \" + str(text_length))\n",
        "print(\"No. of characters = \" + str(char_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wllMB4A3qX29",
        "outputId": "fedf4a07-4dcc-4ef1-d267-15f6e215a25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length = 163006\n",
            "No. of characters = 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "input_strings = []\n",
        "output_strings = []\n",
        "\n",
        "for i in range(len(raw_text) - SEQ_LENGTH):\n",
        "    X_text = raw_text[i: i + SEQ_LENGTH]\n",
        "    X = [char_to_int[char] for char in X_text]\n",
        "    input_strings.append(X)\n",
        "    Y = raw_text[i + SEQ_LENGTH]\n",
        "    output_strings.append(char_to_int[Y])\n",
        "\n",
        "length = len(input_strings)\n",
        "input_strings = np.array(input_strings)\n",
        "input_strings = np.reshape(input_strings, (input_strings.shape[0], input_strings.shape[1], 1))\n",
        "input_strings = input_strings/float(VOCABULARY)\n",
        "\n",
        "output_strings = np.array(output_strings)\n",
        "output_strings = np_utils.to_categorical(output_strings)\n",
        "print(input_strings.shape)\n",
        "print(output_strings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T29-YQNdqZkT",
        "outputId": "e958ef8a-f133-4e3b-a083-6ee521f2400c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(162906, 100, 1)\n",
            "(162906, 54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = buildmodel(VOCABULARY)\n",
        "filepath=\"saved_models/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "xbI1LA_mrHTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(input_strings, output_strings, epochs = 50, batch_size = 128, callbacks = callbacks_list)\n",
        "\n",
        "filename = 'saved_models/weights-improvement-01-3.0451.hdf5'\n",
        "model = buildmodel(VOCABULARY)\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhO_Jk5drOBG",
        "outputId": "7114789e-196d-4daf-8171-09d849c563db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.8767\n",
            "Epoch 00001: loss improved from inf to 2.87666, saving model to saved_models/weights-improvement-01-2.8767.hdf5\n",
            "1273/1273 [==============================] - 2297s 2s/step - loss: 2.8767\n",
            "Epoch 2/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.5967\n",
            "Epoch 00002: loss improved from 2.87666 to 2.59669, saving model to saved_models/weights-improvement-02-2.5967.hdf5\n",
            "1273/1273 [==============================] - 2372s 2s/step - loss: 2.5967\n",
            "Epoch 3/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.4166\n",
            "Epoch 00003: loss improved from 2.59669 to 2.41659, saving model to saved_models/weights-improvement-03-2.4166.hdf5\n",
            "1273/1273 [==============================] - 2370s 2s/step - loss: 2.4166\n",
            "Epoch 4/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.2656\n",
            "Epoch 00004: loss improved from 2.41659 to 2.26560, saving model to saved_models/weights-improvement-04-2.2656.hdf5\n",
            "1273/1273 [==============================] - 2389s 2s/step - loss: 2.2656\n",
            "Epoch 5/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.1480\n",
            "Epoch 00005: loss improved from 2.26560 to 2.14796, saving model to saved_models/weights-improvement-05-2.1480.hdf5\n",
            "1273/1273 [==============================] - 2288s 2s/step - loss: 2.1480\n",
            "Epoch 6/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 2.0541\n",
            "Epoch 00006: loss improved from 2.14796 to 2.05414, saving model to saved_models/weights-improvement-06-2.0541.hdf5\n",
            "1273/1273 [==============================] - 2253s 2s/step - loss: 2.0541\n",
            "Epoch 7/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 1.9769\n",
            "Epoch 00007: loss improved from 2.05414 to 1.97685, saving model to saved_models/weights-improvement-07-1.9769.hdf5\n",
            "1273/1273 [==============================] - 2290s 2s/step - loss: 1.9769\n",
            "Epoch 8/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 1.9112\n",
            "Epoch 00008: loss improved from 1.97685 to 1.91118, saving model to saved_models/weights-improvement-08-1.9112.hdf5\n",
            "1273/1273 [==============================] - 2312s 2s/step - loss: 1.9112\n",
            "Epoch 9/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 1.8520\n",
            "Epoch 00009: loss improved from 1.91118 to 1.85195, saving model to saved_models/weights-improvement-09-1.8520.hdf5\n",
            "1273/1273 [==============================] - 2267s 2s/step - loss: 1.8520\n",
            "Epoch 10/50\n",
            "1273/1273 [==============================] - ETA: 0s - loss: 1.8032\n",
            "Epoch 00010: loss improved from 1.85195 to 1.80320, saving model to saved_models/weights-improvement-10-1.8032.hdf5\n",
            "1273/1273 [==============================] - 2271s 2s/step - loss: 1.8032\n",
            "Epoch 11/50\n",
            " 813/1273 [==================>...........] - ETA: 13:44 - loss: 1.7580"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_text = ' the sun did not shine, it was too wet to play, so we sat in the house all that cold, cold wet day. ' # we sat here we two and we said how we wish we had something to do.\n",
        "initial_text = [char_to_int[c] for c in initial_text]\n",
        "\n",
        "GENERATED_LENGTH = 10\n",
        "test_text = initial_text\n",
        "generated_text = []\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "for i in range(10):\n",
        "    X = np.reshape(test_text, (1, SEQ_LENGTH, 1))\n",
        "    next_character = model.predict(X/float(VOCABULARY))\n",
        "    index = np.argmax(next_character)\n",
        "    generated_text.append(int_to_char[index])\n",
        "    test_text.append(index)\n",
        "    test_text = test_text[1:]\n",
        "\n",
        "print(''.join(generated_text))"
      ],
      "metadata": {
        "id": "MT8NxBI3stVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}